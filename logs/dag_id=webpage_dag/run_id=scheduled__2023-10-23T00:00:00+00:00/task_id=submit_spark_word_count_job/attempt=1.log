[2023-10-25T10:08:52.486+0530] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-23T00:00:00+00:00 [queued]>
[2023-10-25T10:08:52.490+0530] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-23T00:00:00+00:00 [queued]>
[2023-10-25T10:08:52.491+0530] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2023-10-25T10:08:52.499+0530] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): submit_spark_word_count_job> on 2023-10-23 00:00:00+00:00
[2023-10-25T10:08:52.502+0530] {standard_task_runner.py:57} INFO - Started process 8296 to run task
[2023-10-25T10:08:52.504+0530] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'webpage_dag', 'submit_spark_word_count_job', 'scheduled__2023-10-23T00:00:00+00:00', '--job-id', '836', '--raw', '--subdir', 'DAGS_FOLDER/word_count_url.py', '--cfg-path', '/tmp/tmp6_jtfc96']
[2023-10-25T10:08:52.505+0530] {standard_task_runner.py:85} INFO - Job 836: Subtask submit_spark_word_count_job
[2023-10-25T10:08:52.525+0530] {task_command.py:415} INFO - Running <TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-23T00:00:00+00:00 [running]> on host harika-Latitude-5511
[2023-10-25T10:08:52.569+0530] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='harikasree2225@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='webpage_dag' AIRFLOW_CTX_TASK_ID='submit_spark_word_count_job' AIRFLOW_CTX_EXECUTION_DATE='2023-10-23T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-10-23T00:00:00+00:00'
[2023-10-25T10:08:52.573+0530] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-10-25T10:08:52.574+0530] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master local --name PythonWordCount --queue root.default /home/harika/sparkjobs/word_count_extended.py /home/harika/webdata/web_page_data/content.txt
[2023-10-25T10:08:53.881+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:53 WARN Utils: Your hostname, harika-Latitude-5511 resolves to a loopback address: 127.0.1.1; using 192.168.29.76 instead (on interface wlo1)
[2023-10-25T10:08:53.884+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-10-25T10:08:55.328+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkContext: Running Spark version 3.5.0
[2023-10-25T10:08:55.328+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkContext: OS info Linux, 6.2.0-34-generic, amd64
[2023-10-25T10:08:55.328+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkContext: Java version 17.0.1
[2023-10-25T10:08:55.378+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-10-25T10:08:55.470+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO ResourceUtils: ==============================================================
[2023-10-25T10:08:55.470+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-10-25T10:08:55.470+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO ResourceUtils: ==============================================================
[2023-10-25T10:08:55.471+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkContext: Submitted application: PythonWordCount
[2023-10-25T10:08:55.491+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-10-25T10:08:55.499+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO ResourceProfile: Limiting resource is cpu
[2023-10-25T10:08:55.499+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-10-25T10:08:55.551+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SecurityManager: Changing view acls to: harika
[2023-10-25T10:08:55.551+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SecurityManager: Changing modify acls to: harika
[2023-10-25T10:08:55.552+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SecurityManager: Changing view acls groups to:
[2023-10-25T10:08:55.553+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SecurityManager: Changing modify acls groups to:
[2023-10-25T10:08:55.553+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: harika; groups with view permissions: EMPTY; users with modify permissions: harika; groups with modify permissions: EMPTY
[2023-10-25T10:08:55.737+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO Utils: Successfully started service 'sparkDriver' on port 44695.
[2023-10-25T10:08:55.764+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkEnv: Registering MapOutputTracker
[2023-10-25T10:08:55.795+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkEnv: Registering BlockManagerMaster
[2023-10-25T10:08:55.811+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-10-25T10:08:55.811+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-10-25T10:08:55.815+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-10-25T10:08:55.833+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9108d614-65ad-4295-afb5-9c5e75d60d8d
[2023-10-25T10:08:55.844+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-10-25T10:08:55.856+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-10-25T10:08:55.961+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-10-25T10:08:56.013+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-10-25T10:08:56.091+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Executor: Starting executor ID driver on host 192.168.29.76
[2023-10-25T10:08:56.091+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Executor: OS info Linux, 6.2.0-34-generic, amd64
[2023-10-25T10:08:56.091+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Executor: Java version 17.0.1
[2023-10-25T10:08:56.096+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-10-25T10:08:56.097+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@31e40465 for default.
[2023-10-25T10:08:56.111+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45903.
[2023-10-25T10:08:56.111+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO NettyBlockTransferService: Server created on 192.168.29.76:45903
[2023-10-25T10:08:56.112+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-10-25T10:08:56.117+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.29.76, 45903, None)
[2023-10-25T10:08:56.119+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.29.76:45903 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.29.76, 45903, None)
[2023-10-25T10:08:56.121+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.29.76, 45903, None)
[2023-10-25T10:08:56.122+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.29.76, 45903, None)
[2023-10-25T10:08:56.419+0530] {spark_submit.py:491} INFO - Input file location: /home/harika/webdata/web_page_data/content.txt
[2023-10-25T10:08:56.473+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-10-25T10:08:56.480+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:56 INFO SharedState: Warehouse path is 'file:/home/harika/spark-warehouse'.
[2023-10-25T10:08:57.420+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:57 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2023-10-25T10:08:58.078+0530] {spark_submit.py:491} INFO - Showing records of dataframe read from file
[2023-10-25T10:08:58.111+0530] {spark_submit.py:491} INFO - root
[2023-10-25T10:08:58.112+0530] {spark_submit.py:491} INFO - |-- value: string (nullable = true)
[2023-10-25T10:08:58.112+0530] {spark_submit.py:491} INFO - 
[2023-10-25T10:08:58.112+0530] {spark_submit.py:491} INFO - None
[2023-10-25T10:08:58.464+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO FileSourceStrategy: Pushed Filters:
[2023-10-25T10:08:58.473+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO FileSourceStrategy: Post-Scan Filters:
[2023-10-25T10:08:58.872+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO CodeGenerator: Code generated in 130.306066 ms
[2023-10-25T10:08:58.908+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.2 KiB, free 434.2 MiB)
[2023-10-25T10:08:58.951+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 434.2 MiB)
[2023-10-25T10:08:58.953+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.29.76:45903 (size: 34.2 KiB, free: 434.4 MiB)
[2023-10-25T10:08:58.956+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0
[2023-10-25T10:08:58.963+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212058 bytes, open cost is considered as scanning 4194304 bytes.
[2023-10-25T10:08:59.045+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2023-10-25T10:08:59.055+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-25T10:08:59.056+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2023-10-25T10:08:59.056+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Parents of final stage: List()
[2023-10-25T10:08:59.057+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Missing parents: List()
[2023-10-25T10:08:59.060+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-25T10:08:59.117+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.8 KiB, free 434.2 MiB)
[2023-10-25T10:08:59.121+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)
[2023-10-25T10:08:59.122+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.29.76:45903 (size: 6.1 KiB, free: 434.4 MiB)
[2023-10-25T10:08:59.122+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2023-10-25T10:08:59.132+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-25T10:08:59.132+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-10-25T10:08:59.160+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.29.76, executor driver, partition 0, PROCESS_LOCAL, 8231 bytes)
[2023-10-25T10:08:59.168+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-10-25T10:08:59.242+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO CodeGenerator: Code generated in 8.977865 ms
[2023-10-25T10:08:59.245+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO FileScanRDD: Reading File path: file:///home/harika/webdata/web_page_data/content.txt, range: 0-17754, partition values: [empty row]
[2023-10-25T10:08:59.258+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO CodeGenerator: Code generated in 8.860689 ms
[2023-10-25T10:08:59.306+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3484 bytes result sent to driver
[2023-10-25T10:08:59.313+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 161 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-25T10:08:59.315+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-10-25T10:08:59.318+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.246 s
[2023-10-25T10:08:59.320+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-10-25T10:08:59.321+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-10-25T10:08:59.322+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.277025 s
[2023-10-25T10:08:59.827+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.29.76:45903 in memory (size: 6.1 KiB, free: 434.4 MiB)
[2023-10-25T10:08:59.989+0530] {spark_submit.py:491} INFO - 23/10/25 10:08:59 INFO CodeGenerator: Code generated in 8.543254 ms
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - |value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.051+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |Apache Spark: Introduction, Examples and Use Cases | Toptal®                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |®Top 3%WhyClientsEnterpriseCommunityBlogAbout UsApply as a DeveloperHire a DeveloperLog In®DevelopersTop 3%WhyClientsEnterpriseCommunityBlogAbout UsApply as a DeveloperHire a DeveloperLog InEngineeringEngineeringDesignFinanceProjectsProductToptal InsightsSearchData Science and Databases8 minute readIntroduction to Apache Spark with Examples and Use CasesIn this post, Toptal engineer Radek Ostrowski introduces Apache Spark – fast, easy-to-use, and flexible big data processing.  Billed as offering “lightning fast cluster computing”, the Spark technology stack incorporates a comprehensive set of capabilities, including SparkSQL, Spark Streaming, MLlib (for machine learning), and GraphX. Spark may very well be the “child prodigy of big data”, rapidly gaining a dominant position in the complex world of big data processing.|
[2023-10-25T10:09:00.052+0530] {spark_submit.py:491} INFO - |Toptalauthors are vetted experts in their fields and write on topics in which they have demonstrated experience. All of our content is peer reviewed and validated by Toptal experts in the same field.In this post, Toptal engineer Radek Ostrowski introduces Apache Spark – fast, easy-to-use, and flexible big data processing.  Billed as offering “lightning fast cluster computing”, the Spark technology stack incorporates a comprehensive set of capabilities, including SparkSQL, Spark Streaming, MLlib (for machine learning), and GraphX. Spark may very well be the “child prodigy of big data”, rapidly gaining a dominant position in the complex world of big data processing.                                                                                                                                                             |
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - |Toptalauthors are vetted experts in their fields and write on topics in which they have demonstrated experience. All of our content is peer reviewed and validated by Toptal experts in the same field.By Radek OstrowskiVerified Expert in EngineeringRadek is a blockchain engineer with an interest in Ethereum smart contracts. He also has extensive experience in machine learning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - |ExpertiseSparkApacheBig DataPreviously AtShareShareI first heard of Spark in late 2013 when I became interested in Scala, the language in which Spark is written. Some time later, I did a fun data science project trying to predict survival on the Titanic. This turned out to be a great way to get further introduced to Spark concepts and programming. I highly recommend it for any aspiring Spark developers looking for a place to get started.                                                                                                                                                                                                                                                                                                                                                                                                    |
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - |Today, Spark is being adopted by major players like Amazon, eBay, and Yahoo! Many organizations run Spark on clusters with thousands of nodes. According to the Spark FAQ, the largest known cluster has over 8000 nodes. Indeed, Spark is a technology well worth taking note of and learning about.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - only showing top 20 rows
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - 
[2023-10-25T10:09:00.053+0530] {spark_submit.py:491} INFO - None
[2023-10-25T10:09:00.450+0530] {spark_submit.py:491} INFO - Records of dataframe after processing text df
[2023-10-25T10:09:00.450+0530] {spark_submit.py:491} INFO - root
[2023-10-25T10:09:00.451+0530] {spark_submit.py:491} INFO - |-- words_clean: array (nullable = true)
[2023-10-25T10:09:00.451+0530] {spark_submit.py:491} INFO - |    |-- element: string (containsNull = true)
[2023-10-25T10:09:00.451+0530] {spark_submit.py:491} INFO - 
[2023-10-25T10:09:00.451+0530] {spark_submit.py:491} INFO - None
[2023-10-25T10:09:00.533+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO FileSourceStrategy: Pushed Filters:
[2023-10-25T10:09:00.534+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO FileSourceStrategy: Post-Scan Filters:
[2023-10-25T10:09:00.614+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.2 KiB, free 434.0 MiB)
[2023-10-25T10:09:00.628+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.9 MiB)
[2023-10-25T10:09:00.629+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.29.76:45903 (size: 34.2 KiB, free: 434.3 MiB)
[2023-10-25T10:09:00.631+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[2023-10-25T10:09:00.632+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212058 bytes, open cost is considered as scanning 4194304 bytes.
[2023-10-25T10:09:00.646+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.29.76:45903 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2023-10-25T10:09:00.655+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Registering RDD 7 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2023-10-25T10:09:00.658+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-25T10:09:00.658+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2023-10-25T10:09:00.658+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Parents of final stage: List()
[2023-10-25T10:09:00.659+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Missing parents: List()
[2023-10-25T10:09:00.660+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-25T10:09:00.678+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2023-10-25T10:09:00.681+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)
[2023-10-25T10:09:00.682+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.29.76:45903 (size: 6.1 KiB, free: 434.4 MiB)
[2023-10-25T10:09:00.682+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[2023-10-25T10:09:00.684+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-25T10:09:00.684+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-10-25T10:09:00.687+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.29.76, executor driver, partition 0, PROCESS_LOCAL, 8220 bytes)
[2023-10-25T10:09:00.687+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-10-25T10:09:00.718+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO FileScanRDD: Reading File path: file:///home/harika/webdata/web_page_data/content.txt, range: 0-17754, partition values: [empty row]
[2023-10-25T10:09:00.770+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1805 bytes result sent to driver
[2023-10-25T10:09:00.772+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 87 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-25T10:09:00.772+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-10-25T10:09:00.775+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.112 s
[2023-10-25T10:09:00.775+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: looking for newly runnable stages
[2023-10-25T10:09:00.776+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: running: Set()
[2023-10-25T10:09:00.776+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: waiting: Set()
[2023-10-25T10:09:00.777+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: failed: Set()
[2023-10-25T10:09:00.852+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO CodeGenerator: Code generated in 40.177609 ms
[2023-10-25T10:09:00.901+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2023-10-25T10:09:00.903+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-25T10:09:00.903+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2023-10-25T10:09:00.903+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2023-10-25T10:09:00.903+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Missing parents: List()
[2023-10-25T10:09:00.904+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-25T10:09:00.912+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 39.2 KiB, free 434.1 MiB)
[2023-10-25T10:09:00.916+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 434.1 MiB)
[2023-10-25T10:09:00.917+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.29.76:45903 (size: 17.6 KiB, free: 434.3 MiB)
[2023-10-25T10:09:00.917+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.29.76:45903 in memory (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-25T10:09:00.917+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
[2023-10-25T10:09:00.918+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-25T10:09:00.918+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-10-25T10:09:00.923+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (192.168.29.76, executor driver, partition 0, NODE_LOCAL, 7615 bytes)
[2023-10-25T10:09:00.924+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
[2023-10-25T10:09:00.978+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO ShuffleBlockFetcherIterator: Getting 1 (3.9 KiB) non-empty blocks including 1 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-25T10:09:00.980+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2023-10-25T10:09:01.005+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO CodeGenerator: Code generated in 21.518888 ms
[2023-10-25T10:09:01.021+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO CodeGenerator: Code generated in 6.310994 ms
[2023-10-25T10:09:01.047+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO CodeGenerator: Code generated in 19.510415 ms
[2023-10-25T10:09:01.063+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO CodeGenerator: Code generated in 10.018601 ms
[2023-10-25T10:09:01.078+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4456 bytes result sent to driver
[2023-10-25T10:09:01.080+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 159 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-25T10:09:01.080+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-10-25T10:09:01.081+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.173 s
[2023-10-25T10:09:01.081+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-10-25T10:09:01.082+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-10-25T10:09:01.082+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.180792 s
[2023-10-25T10:09:01.089+0530] {spark_submit.py:491} INFO - +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-25T10:09:01.090+0530] {spark_submit.py:491} INFO - |words_clean                                                                                                                                                                                                                                                                                                                   |
[2023-10-25T10:09:01.090+0530] {spark_submit.py:491} INFO - +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-25T10:09:01.090+0530] {spark_submit.py:491} INFO - |[spark, provides, faster, general, data, processing, platform, spark, lets, run, programs, x, faster, memory, x, faster, disk, hadoop, last, year, spark, took, hadoop, completing, , tb, daytona, graysort, contest, x, faster, one, tenth, number, machines, also, became, fastest, open, source, engine, sorting, petabyte]|
[2023-10-25T10:09:01.090+0530] {spark_submit.py:491} INFO - |[game, industry, processing, discovering, patterns, potential, firehose, realtime, ingame, events, able, respond, immediately, capability, yield, lucrative, business, purposes, player, retention, targeted, advertising, autoadjustment, complexity, level]                                                                 |
[2023-10-25T10:09:01.090+0530] {spark_submit.py:491} INFO - |[spark, introduces, concept, rdd, resilient, distributed, dataset, immutable, faulttolerant, distributed, collection, objects, operated, parallel, rdd, contain, type, object, created, loading, external, dataset, distributing, collection, driver, program]                                                                |
[2023-10-25T10:09:01.090+0530] {spark_submit.py:491} INFO - |[, get, evaluation, metrics]                                                                                                                                                                                                                                                                                                  |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[sqlcontextsqlload, data, local, inpath, examplessrcmainresourceskvtxt, table, src]                                                                                                                                                                                                                                           |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[, clear, default, threshold]                                                                                                                                                                                                                                                                                                 |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[, , score, pointlabel]                                                                                                                                                                                                                                                                                                       |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[ecommerce, industry, realtime, transaction, information, passed, streaming, clustering, algorithm, like, kmeans, collaborative, filtering, like, als, results, even, combined, unstructured, data, sources, customer, comments, product, reviews, used, constantly, improve, adapt, recommendations, time, new, trends]      |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[, , , , , , , , , , , , saveastextfilehdfs]                                                                                                                                                                                                                                                                                  |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[heres, quick, certainly, nowhere, near, exhaustive, sampling, use, cases, require, dealing, velocity, variety, volume, big, data, spark, well, suited]                                                                                                                                                                       |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[val, sqlcontext, , new, orgapachesparksqlhivehivecontextsc]                                                                                                                                                                                                                                                                  |
[2023-10-25T10:09:01.091+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.092+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.093+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                            |
[2023-10-25T10:09:01.093+0530] {spark_submit.py:491} INFO - +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-25T10:09:01.093+0530] {spark_submit.py:491} INFO - only showing top 20 rows
[2023-10-25T10:09:01.093+0530] {spark_submit.py:491} INFO - 
[2023-10-25T10:09:01.093+0530] {spark_submit.py:491} INFO - None
[2023-10-25T10:09:01.108+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO FileSourceStrategy: Pushed Filters:
[2023-10-25T10:09:01.109+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO FileSourceStrategy: Post-Scan Filters:
[2023-10-25T10:09:01.117+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.2 KiB, free 433.9 MiB)
[2023-10-25T10:09:01.126+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.29.76:45903 in memory (size: 17.6 KiB, free: 434.4 MiB)
[2023-10-25T10:09:01.132+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.9 MiB)
[2023-10-25T10:09:01.133+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.29.76:45903 (size: 34.2 KiB, free: 434.3 MiB)
[2023-10-25T10:09:01.134+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0
[2023-10-25T10:09:01.135+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212058 bytes, open cost is considered as scanning 4194304 bytes.
[2023-10-25T10:09:01.140+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Registering RDD 14 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2023-10-25T10:09:01.140+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Got map stage job 3 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-25T10:09:01.140+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0)
[2023-10-25T10:09:01.140+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Parents of final stage: List()
[2023-10-25T10:09:01.141+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Missing parents: List()
[2023-10-25T10:09:01.141+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-25T10:09:01.144+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2023-10-25T10:09:01.145+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 433.9 MiB)
[2023-10-25T10:09:01.146+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.29.76:45903 (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-25T10:09:01.147+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[2023-10-25T10:09:01.147+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-25T10:09:01.148+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-10-25T10:09:01.152+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (192.168.29.76, executor driver, partition 0, PROCESS_LOCAL, 8220 bytes)
[2023-10-25T10:09:01.153+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[2023-10-25T10:09:01.159+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO FileScanRDD: Reading File path: file:///home/harika/webdata/web_page_data/content.txt, range: 0-17754, partition values: [empty row]
[2023-10-25T10:09:01.178+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1805 bytes result sent to driver
[2023-10-25T10:09:01.181+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 31 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-25T10:09:01.181+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-10-25T10:09:01.182+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.040 s
[2023-10-25T10:09:01.182+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: looking for newly runnable stages
[2023-10-25T10:09:01.182+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: running: Set()
[2023-10-25T10:09:01.183+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: waiting: Set()
[2023-10-25T10:09:01.183+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: failed: Set()
[2023-10-25T10:09:01.183+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.29.76:45903 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2023-10-25T10:09:01.218+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO CodeGenerator: Code generated in 22.23772 ms
[2023-10-25T10:09:01.284+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO SparkContext: Starting job: countByValue at /home/harika/sparkjobs/word_count_extended.py:53
[2023-10-25T10:09:01.285+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Got job 4 (countByValue at /home/harika/sparkjobs/word_count_extended.py:53) with 4 output partitions
[2023-10-25T10:09:01.286+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Final stage: ResultStage 6 (countByValue at /home/harika/sparkjobs/word_count_extended.py:53)
[2023-10-25T10:09:01.286+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2023-10-25T10:09:01.286+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Missing parents: List()
[2023-10-25T10:09:01.286+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[20] at countByValue at /home/harika/sparkjobs/word_count_extended.py:53), which has no missing parents
[2023-10-25T10:09:01.291+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 47.4 KiB, free 434.1 MiB)
[2023-10-25T10:09:01.295+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 434.1 MiB)
[2023-10-25T10:09:01.296+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.29.76:45903 (size: 22.1 KiB, free: 434.3 MiB)
[2023-10-25T10:09:01.296+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.29.76:45903 in memory (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-25T10:09:01.297+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
[2023-10-25T10:09:01.297+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 6 (PythonRDD[20] at countByValue at /home/harika/sparkjobs/word_count_extended.py:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2023-10-25T10:09:01.297+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSchedulerImpl: Adding task set 6.0 with 4 tasks resource profile 0
[2023-10-25T10:09:01.299+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (192.168.29.76, executor driver, partition 0, NODE_LOCAL, 7615 bytes)
[2023-10-25T10:09:01.300+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)
[2023-10-25T10:09:01.333+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO ShuffleBlockFetcherIterator: Getting 1 (3.9 KiB) non-empty blocks including 1 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-25T10:09:01.333+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-10-25T10:09:01.357+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO CodeGenerator: Code generated in 23.607289 ms
[2023-10-25T10:09:01.973+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO PythonRunner: Times: total = 604, boot = 496, init = 108, finish = 0
[2023-10-25T10:09:01.979+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 7957 bytes result sent to driver
[2023-10-25T10:09:01.980+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 5) (192.168.29.76, executor driver, partition 1, NODE_LOCAL, 7615 bytes)
[2023-10-25T10:09:01.981+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 682 ms on 192.168.29.76 (executor driver) (1/4)
[2023-10-25T10:09:01.981+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO Executor: Running task 1.0 in stage 6.0 (TID 5)
[2023-10-25T10:09:01.982+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50305
[2023-10-25T10:09:01.990+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO ShuffleBlockFetcherIterator: Getting 1 (4.3 KiB) non-empty blocks including 1 (4.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-25T10:09:01.991+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-10-25T10:09:02.084+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO PythonRunner: Times: total = 92, boot = -8, init = 100, finish = 0
[2023-10-25T10:09:02.086+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO Executor: Finished task 1.0 in stage 6.0 (TID 5). 8012 bytes result sent to driver
[2023-10-25T10:09:02.087+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 6) (192.168.29.76, executor driver, partition 2, NODE_LOCAL, 7615 bytes)
[2023-10-25T10:09:02.088+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 5) in 107 ms on 192.168.29.76 (executor driver) (2/4)
[2023-10-25T10:09:02.088+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO Executor: Running task 2.0 in stage 6.0 (TID 6)
[2023-10-25T10:09:02.098+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-25T10:09:02.098+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-10-25T10:09:02.179+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO PythonRunner: Times: total = 80, boot = -1, init = 80, finish = 1
[2023-10-25T10:09:02.181+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO Executor: Finished task 2.0 in stage 6.0 (TID 6). 7241 bytes result sent to driver
[2023-10-25T10:09:02.182+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 7) (192.168.29.76, executor driver, partition 3, NODE_LOCAL, 7615 bytes)
[2023-10-25T10:09:02.183+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO Executor: Running task 3.0 in stage 6.0 (TID 7)
[2023-10-25T10:09:02.183+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 6) in 97 ms on 192.168.29.76 (executor driver) (3/4)
[2023-10-25T10:09:02.194+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO ShuffleBlockFetcherIterator: Getting 1 (3.9 KiB) non-empty blocks including 1 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-25T10:09:02.194+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-10-25T10:09:02.278+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO PythonRunner: Times: total = 83, boot = 1, init = 82, finish = 0
[2023-10-25T10:09:02.281+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO Executor: Finished task 3.0 in stage 6.0 (TID 7). 8325 bytes result sent to driver
[2023-10-25T10:09:02.282+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 7) in 100 ms on 192.168.29.76 (executor driver) (4/4)
[2023-10-25T10:09:02.283+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-10-25T10:09:02.284+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO DAGScheduler: ResultStage 6 (countByValue at /home/harika/sparkjobs/word_count_extended.py:53) finished in 0.995 s
[2023-10-25T10:09:02.284+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-10-25T10:09:02.284+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-10-25T10:09:02.285+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO DAGScheduler: Job 4 finished: countByValue at /home/harika/sparkjobs/word_count_extended.py:53, took 1.000809 s
[2023-10-25T10:09:02.293+0530] {spark_submit.py:491} INFO - defaultdict(<class 'int'>, {'spark': 65, 'provides': 7, 'faster': 5, 'general': 1, 'data': 30, 'processing': 9, 'platform': 1, 'lets': 3, 'run': 9, 'programs': 1, 'x': 3, 'memory': 3, 'disk': 1, 'hadoop': 5, 'last': 1, 'year': 1, 'took': 1, 'completing': 1, '': 190, 'tb': 1, 'daytona': 1, 'graysort': 1, 'contest': 1, 'one': 3, 'tenth': 1, 'number': 2, 'machines': 1, 'also': 9, 'became': 2, 'fastest': 1, 'open': 1, 'source': 1, 'engine': 4, 'sorting': 1, 'petabyte': 1, 'game': 1, 'industry': 3, 'discovering': 1, 'patterns': 1, 'potential': 2, 'firehose': 1, 'realtime': 4, 'ingame': 1, 'events': 1, 'able': 1, 'respond': 1, 'immediately': 1, 'capability': 1, 'yield': 2, 'lucrative': 1, 'business': 1, 'purposes': 1, 'player': 1, 'retention': 1, 'targeted': 1, 'advertising': 1, 'autoadjustment': 1, 'complexity': 1, 'level': 1, 'introduces': 3, 'concept': 1, 'rdd': 7, 'resilient': 1, 'distributed': 3, 'dataset': 3, 'immutable': 1, 'faulttolerant': 1, 'collection': 2, 'objects': 1, 'operated': 1, 'parallel': 2, 'contain': 1, 'type': 1, 'object': 1, 'created': 1, 'loading': 1, 'external': 2, 'distributing': 2, 'driver': 2, 'program': 2, 'get': 4, 'evaluation': 1, 'metrics': 2, 'sqlcontextsqlload': 1, 'local': 1, 'inpath': 1, 'examplessrcmainresourceskvtxt': 1, 'table': 3, 'src': 3, 'clear': 1, 'default': 2, 'threshold': 1, 'score': 2, 'pointlabel': 1, 'ecommerce': 1, 'transaction': 1, 'information': 5, 'passed': 2, 'streaming': 14, 'clustering': 3, 'algorithm': 2, 'like': 10, 'kmeans': 2, 'collaborative': 2, 'filtering': 2, 'als': 1, 'results': 5, 'even': 2, 'combined': 1, 'unstructured': 2, 'sources': 4, 'customer': 1, 'comments': 1, 'product': 2, 'reviews': 1, 'used': 6, 'constantly': 1, 'improve': 1, 'adapt': 1, 'recommendations': 1, 'time': 7, 'new': 5, 'trends': 1, 'saveastextfilehdfs': 1, 'heres': 1, 'quick': 1, 'certainly': 1, 'nowhere': 1, 'near': 1, 'exhaustive': 1, 'sampling': 1, 'use': 13, 'cases': 5, 'require': 1, 'dealing': 1, 'velocity': 1, 'variety': 1, 'volume': 1, 'big': 10, 'well': 8, 'suited': 1, 'val': 12, 'sqlcontext': 2, 'orgapachesparksqlhivehivecontextsc': 2, 'authors': 2, 'vetted': 3, 'experts': 6, 'fields': 3, 'write': 4, 'topics': 3, 'demonstrated': 3, 'experience': 6, 'content': 3, 'peer': 3, 'reviewed': 3, 'validated': 3, 'toptal': 12, 'fieldexpertisesparkapachebig': 1, 'datapreviously': 2, 'athire': 1, 'radekworldclass': 1, 'articles': 3, 'delivered': 3, 'weeklyget': 1, 'great': 3, 'contentsubscription': 1, 'implies': 3, 'consent': 3, 'privacy': 3, 'policy': 3, 'twitterutilscreatestream': 1, 'another': 1, 'important': 1, 'aspect': 1, 'learning': 13, 'apache': 21, 'interactive': 1, 'shell': 1, 'repl': 2, 'outofthe': 1, 'box': 1, 'using': 4, 'test': 4, 'outcome': 1, 'line': 2, 'code': 10, 'without': 2, 'first': 6, 'needing': 1, 'execute': 1, 'entire': 2, 'job': 1, 'path': 1, 'working': 1, 'thus': 1, 'much': 2, 'shorter': 1, 'adhoc': 1, 'analysis': 5, 'made': 1, 'possible': 3, 'sendemail': 1, 'custom': 1, 'function': 1, 'interacting': 1, 'storage': 1, 'systems': 1, 'auroc': 2, 'metricsareaunderroc': 1, 'youspark': 1, 'helps': 2, 'simplify': 2, 'challenging': 2, 'computationally': 2, 'intensive': 2, 'task': 2, 'high': 2, 'volumes': 2, 'archived': 3, 'reading': 1, 'bloghow': 1, 'docker': 1, 'hackathon': 1, 'build': 2, 'weather': 1, 'appapache': 1, 'tutorial': 1, 'identifying': 1, 'trending': 2, 'twitter': 4, 'hashtagsai': 1, 'project': 8, 'development': 2, 'managers': 1, 'preparestreamline': 1, 'software': 1, 'integration': 1, 'camel': 1, 'tutorialthe': 1, 'ultimate': 1, 'ens': 1, 'app': 2, 'tutorialunderstanding': 1, 'basicswhat': 1, 'sparkspark': 1, 'advertised': 2, 'lightning': 4, 'fast': 6, 'cluster': 8, 'computing': 4, 'thriving': 2, 'opensource': 2, 'community': 2, 'active': 2, 'moment': 2, 'actions': 1, 'operations': 5, 'reduce': 1, 'count': 2, 'return': 2, 'value': 2, 'running': 1, 'computation': 1, 'mapword': 1, 'word': 2, 'reducebykey': 1, 'transformations': 4, 'lazy': 1, 'meaning': 1, 'compute': 2, 'right': 1, 'away': 2, 'instead': 1, 'remember': 1, 'operation': 2, 'performed': 3, 'eg': 2, 'file': 3, 'actually': 1, 'computed': 1, 'action': 3, 'called': 1, 'result': 3, 'returned': 1, 'design': 1, 'enables': 1, 'efficiently': 1, 'example': 7, 'transformed': 2, 'various': 4, 'ways': 1, 'process': 1, 'rather': 1, 'work': 3, 'answered': 1, 'question': 1, 'think': 2, 'kind': 1, 'problems': 1, 'challenges': 1, 'effectively': 1, 'may': 4, 'recomputed': 1, 'however': 1, 'persist': 2, 'cache': 1, 'method': 1, 'case': 2, 'keep': 1, 'elements': 1, 'around': 2, 'access': 1, 'next': 3, 'query': 4, 'graphx': 5, 'library': 4, 'manipulating': 1, 'graphs': 1, 'performing': 1, 'graphparallel': 1, 'uniform': 1, 'tool': 2, 'etl': 1, 'exploratory': 1, 'iterative': 1, 'graph': 4, 'computations': 1, 'apart': 1, 'builtin': 1, 'manipulation': 1, 'common': 1, 'algorithms': 4, 'pagerank': 1, 'sparkcontexttextfilehdfs': 1, 'finance': 2, 'security': 2, 'stack': 4, 'applied': 1, 'fraud': 1, 'intrusion': 1, 'detection': 3, 'system': 1, 'riskbased': 1, 'authentication': 1, 'achieve': 1, 'topnotch': 1, 'harvesting': 1, 'huge': 1, 'amounts': 1, 'logs': 1, 'combining': 1, 'breaches': 1, 'compromised': 1, 'accounts': 1, 'see': 2, 'httpshaveibeenpwnedcom': 1, 'connectionrequest': 1, 'ip': 1, 'geolocation': 1, 'supports': 2, 'real': 1, 'production': 1, 'web': 1, 'server': 1, 'log': 1, 'files': 1, 'flume': 1, 'hdfss': 1, 'social': 1, 'media': 1, 'messaging': 1, 'queues': 1, 'kafka': 1, 'hood': 1, 'receives': 1, 'input': 1, 'streams': 1, 'divides': 1, 'batches': 2, 'processed': 1, 'generate': 1, 'final': 1, 'stream': 2, 'depicted': 1, 'scheduling': 1, 'monitoring': 1, 'jobs': 1, 'came': 1, 'across': 1, 'article': 6, 'recently': 1, 'experiment': 1, 'detect': 2, 'earthquake': 11, 'analyzing': 1, 'interestingly': 1, 'shown': 1, 'technique': 1, 'likely': 1, 'inform': 1, 'japan': 2, 'quicker': 1, 'meteorological': 1, 'agency': 1, 'though': 1, 'different': 1, 'technology': 4, 'put': 1, 'simplified': 1, 'snippets': 1, 'glue': 1, 'event': 1, 'mlutilsloadlibsvmfilesc': 1, 'sampleearthquatetweetstxt': 1, 'sqlcontextsqlfrom': 2, 'earthquakewarningusers': 1, 'select': 2, 'firstname': 1, 'lastname': 1, 'city': 1, 'email': 3, 'clusters': 2, 'managed': 1, 'yarn': 1, 'mesos': 1, 'standalone': 1, 'language': 3, 'written': 4, 'inapache': 1, 'scala': 4, 'splits': 2, 'datarandomsplitarray': 1, 'seed': 1, 'l': 1, 'sum': 1, 'structured': 1, 'seamlessly': 2, 'integrating': 1, 'relevant': 2, 'complex': 3, 'capabilities': 3, 'machine': 10, 'brings': 1, 'masses': 1, 'check': 2, 'raw': 1, 'scores': 1, 'set': 4, 'additional': 2, 'key': 3, 'features': 1, 'include': 2, 'semantic': 1, 'tweets': 6, 'determine': 1, 'appear': 1, 'referencing': 1, 'current': 1, 'occurrence': 1, 'shaking': 2, 'consider': 1, 'positive': 2, 'matches': 2, 'whereas': 2, 'attending': 1, 'conference': 1, 'yesterday': 1, 'scary': 1, 'paper': 1, 'support': 4, 'vector': 1, 'svm': 1, 'purpose': 2, 'try': 1, 'version': 1, 'resulting': 1, 'mllib': 7, 'look': 2, 'following': 1, 'top': 3, 'whyclientsenterprisecommunityblogabout': 2, 'usapply': 2, 'developerhire': 2, 'developerlog': 2, 'indeveloperstop': 1, 'inengineeringengineeringdesignfinanceprojectsproducttoptal': 1, 'insightssearchdata': 1, 'science': 4, 'databases': 1, 'minute': 1, 'readintroduction': 1, 'examples': 3, 'casesin': 1, 'post': 2, 'engineer': 5, 'radek': 3, 'ostrowski': 2, 'easytouse': 2, 'flexible': 2, 'billed': 2, 'offering': 2, 'incorporates': 2, 'comprehensive': 2, 'including': 3, 'sparksql': 6, 'child': 2, 'prodigy': 2, 'rapidly': 2, 'gaining': 2, 'dominant': 2, 'position': 2, 'world': 3, 'dataprocessing': 2, 'sqlcontextsqlcreate': 1, 'exists': 1, 'int': 1, 'string': 1, 'makes': 2, 'quickly': 1, 'highlevel': 1, 'operators': 1, 'disposal': 1, 'demonstrate': 1, 'hello': 1, 'bigdata': 1, 'java': 2, 'mapreduce': 3, 'lines': 1, 'simply': 1, 'flatmapline': 1, 'linesplit': 1, 'split': 1, 'training': 3, 'filter': 2, 'seem': 1, 'easily': 1, 'follows': 2, 'happy': 1, 'prediction': 1, 'rate': 1, 'model': 3, 'move': 1, 'onto': 1, 'stage': 1, 'react': 2, 'whenever': 1, 'discover': 1, 'need': 1, 'certain': 1, 'ie': 1, 'density': 1, 'defined': 1, 'window': 1, 'described': 1, 'note': 2, 'location': 2, 'services': 1, 'enabled': 1, 'extract': 1, 'armed': 1, 'knowledge': 1, 'existing': 3, 'hive': 5, 'storing': 1, 'users': 1, 'interested': 2, 'receiving': 1, 'notifications': 1, 'retrieve': 1, 'addresses': 1, 'send': 1, 'personalized': 1, 'warning': 1, 'worldclass': 1, 'weeklysign': 2, 'upsubscription': 2, 'numiterations': 2, 'expertisesparkapachebig': 1, 'atsharesharei': 1, 'heard': 1, 'late': 1, 'later': 1, 'fun': 1, 'trying': 1, 'predict': 1, 'survival': 1, 'titanic': 1, 'turned': 2, 'way': 3, 'introduced': 1, 'concepts': 1, 'programming': 1, 'highly': 1, 'recommend': 1, 'aspiring': 1, 'developers': 1, 'looking': 1, 'place': 2, 'started': 1, 'filtergettextcontainsearthquake': 1, 'gettextcontainsshaking': 1, 'splitscache': 1, 'introduction': 3, 'printlnarea': 1, 'roc': 1, 'binaryclassificationmetricsscoreandlabels': 1, 'map': 1, 'join': 1, 'union': 1, 'containing': 1, 'management': 1, 'fault': 1, 'recovery': 1, 'toptalauthors': 2, 'fieldby': 1, 'ostrowskiverified': 3, 'expertin': 3, 'engineeringradek': 1, 'blockchain': 2, 'interest': 2, 'ethereum': 2, 'smart': 2, 'contracts': 2, 'extensive': 2, 'valuecollectforeachprintln': 1, 'articlesengineeringdata': 1, 'databasesadvantages': 1, 'ai': 2, 'gpt': 2, 'diffusion': 1, 'models': 2, 'image': 1, 'generationengineeringdata': 1, 'databasesask': 1, 'nlp': 1, 'ethics': 2, 'aiengineeringtechnologyhow': 1, 'jwt': 1, 'nodejs': 1, 'better': 1, 'securityengineeringweb': 1, 'frontendnextjs': 1, 'vs': 1, 'comparative': 1, 'tutorialsee': 1, 'related': 1, 'talentspark': 1, 'developersapache': 1, 'developersbig': 2, 'architectshire': 1, 'expert': 2, 'thistopichire': 2, 'nowhire': 1, 'talentradek': 1, 'engineeringread': 1, 'nextengineeringtechnology': 1, 'pillars': 1, 'responsible': 2, 'generative': 1, 'futureworldclass': 1, 'modelclearthreshold': 1, 'extend': 1, 'far': 1, 'beyond': 1, 'earthquakes': 1, 'course': 1, 'today': 1, 'adopted': 1, 'major': 1, 'players': 1, 'amazon': 2, 'ebay': 1, 'yahoo': 1, 'many': 1, 'organizations': 1, 'thousands': 1, 'nodes': 2, 'according': 1, 'faq': 1, 'largest': 1, 'known': 1, 'indeed': 1, 'worth': 1, 'taking': 1, 'scoreandlabels': 1, 'testmap': 1, 'point': 1, 'sc': 2, 'sparkcontext': 2, 'queries': 2, 'expressed': 1, 'hiveql': 1, 'core': 4, 'base': 1, 'largescale': 1, 'fieldin': 1, 'tagsbigdatasparkapachehire': 1, 'nowradek': 1, 'engineeringlocated': 1, 'phuket': 1, 'thailandmember': 1, 'since': 1, 'september': 1, 'authorradek': 1, 'conclusion': 1, 'developersalgorithm': 1, 'developersangular': 1, 'developersaws': 1, 'developersazure': 1, 'architectsblockchain': 1, 'developersbusiness': 1, 'intelligence': 1, 'developersc': 1, 'developerscomputer': 1, 'vision': 1, 'developersdjango': 1, 'developersdocker': 1, 'developerselixir': 1, 'developersgo': 1, 'engineersgraphql': 1, 'developersjenkins': 1, 'developerskotlin': 1, 'developerskubernetes': 1, 'expertsmachine': 1, 'engineersmagento': 1, 'developersnet': 1, 'developersr': 1, 'developersreact': 1, 'native': 1, 'developersruby': 1, 'rails': 1, 'developerssalesforce': 1, 'developerssql': 1, 'developerssys': 1, 'adminstableau': 1, 'developersunreal': 1, 'developersxamarin': 1, 'developersview': 1, 'freelancedevelopersjoin': 1, 'communityhire': 1, 'developer': 1, 'apply': 1, 'developerfooterondemand': 1, 'talenthire': 1, 'freelance': 5, 'developershire': 1, 'designershire': 1, 'expertshire': 1, 'managershire': 1, 'managersstart': 1, 'finish': 1, 'solutionsmanaged': 1, 'deliverymanagement': 1, 'consultingstrategy': 1, 'consultingpeople': 1, 'organization': 1, 'consultinginnovation': 1, 'consultingtechnology': 1, 'servicesapplication': 1, 'servicescloud': 1, 'servicesinformation': 1, 'servicesquality': 1, 'assurance': 1, 'servicesabouttop': 1, 'clientsfreelance': 1, 'jobsspecialized': 1, 'servicesutilities': 1, 'toolsresearch': 1, 'centerabout': 1, 'uscontactcontact': 1, 'uspress': 1, 'centercareersfaqthe': 1, 'worlds': 2, 'talent': 1, 'demandcopyright': 1, 'llcprivacy': 1, 'policywebsite': 1, 'termsaccessibility': 1, 'svmwithsgdtraintraining': 1, 'collectforeachsendemail': 1, 'designed': 1, 'scale': 1, 'classification': 1, 'regression': 2, 'toptals': 1, 'topic': 1, 'linear': 1, 'ordinary': 1, 'least': 1, 'squares': 1, 'mahout': 1, 'already': 1, 'joined': 1, 'forces': 1, 'integrates': 1, 'ecosystem': 1, 'hdfs': 1, 'hbase': 1, 'cassandra': 1, 'etc': 1, 'component': 1, 'querying': 1, 'either': 1, 'via': 2, 'sql': 2, 'originated': 1, 'port': 1, 'integrated': 1, 'addition': 1, 'providing': 1, 'weave': 1, 'powerful': 2, 'compatible': 1, 'modelpredictpointfeatures': 1, 'api': 1, 'closely': 1, 'making': 1, 'easy': 1, 'programmers': 1, 'batch': 1, 'complemented': 1, 'higherlevel': 1, 'libraries': 3, 'application': 1, 'currently': 3, 'detailed': 1, 'extensions': 1, 'rdds': 1, 'two': 1, 'types': 1, 'prepare': 1, 'tweet': 1, 'load': 1, 'libsvm': 1, 'format': 1, 'contains': 1, 'website': 1, 'book': 1, 'lightningfast': 1, 'apis': 1, 'python': 1, 'languages': 1, 'r': 1})
[2023-10-25T10:09:02.294+0530] {spark_submit.py:491} INFO - Writing word counts to CSV file
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - word  count
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 0                190
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 1       spark     65
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 2        data     30
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 3      apache     21
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 4   streaming     14
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 5         use     13
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 6    learning     13
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 7         val     12
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 8      toptal     12
[2023-10-25T10:09:02.304+0530] {spark_submit.py:491} INFO - 9  earthquake     11
[2023-10-25T10:09:02.306+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-10-25T10:09:02.314+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO SparkUI: Stopped Spark web UI at http://192.168.29.76:4040
[2023-10-25T10:09:02.323+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-10-25T10:09:02.331+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO MemoryStore: MemoryStore cleared
[2023-10-25T10:09:02.331+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO BlockManager: BlockManager stopped
[2023-10-25T10:09:02.334+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-10-25T10:09:02.336+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-10-25T10:09:02.340+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:02 INFO SparkContext: Successfully stopped SparkContext
[2023-10-25T10:09:03.372+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:03 INFO ShutdownHookManager: Shutdown hook called
[2023-10-25T10:09:03.372+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-f56cbd66-1fc3-456b-9907-b0412d49ea05
[2023-10-25T10:09:03.374+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-08b64412-4447-4488-a65a-8fb455228789
[2023-10-25T10:09:03.376+0530] {spark_submit.py:491} INFO - 23/10/25 10:09:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-08b64412-4447-4488-a65a-8fb455228789/pyspark-01e4ba1a-82be-4f1d-8e97-85929be380e4
[2023-10-25T10:09:03.419+0530] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=webpage_dag, task_id=submit_spark_word_count_job, execution_date=20231023T000000, start_date=20231025T043852, end_date=20231025T043903
[2023-10-25T10:09:03.437+0530] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-10-25T10:09:03.447+0530] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-10-25T18:23:38.955+0530] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-23T00:00:00+00:00 [queued]>
[2023-10-25T18:23:38.962+0530] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-23T00:00:00+00:00 [queued]>
[2023-10-25T18:23:38.963+0530] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2023-10-25T18:23:38.974+0530] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): submit_spark_word_count_job> on 2023-10-23 00:00:00+00:00
[2023-10-25T18:23:38.979+0530] {standard_task_runner.py:57} INFO - Started process 94077 to run task
[2023-10-25T18:23:38.981+0530] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'webpage_dag', 'submit_spark_word_count_job', 'scheduled__2023-10-23T00:00:00+00:00', '--job-id', '1595', '--raw', '--subdir', 'DAGS_FOLDER/word_count_url.py', '--cfg-path', '/tmp/tmpxzmo25zy']
[2023-10-25T18:23:38.982+0530] {standard_task_runner.py:85} INFO - Job 1595: Subtask submit_spark_word_count_job
[2023-10-25T18:23:39.007+0530] {task_command.py:415} INFO - Running <TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-23T00:00:00+00:00 [running]> on host harika-Latitude-5511
[2023-10-25T18:23:39.054+0530] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='harikasree2225@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='webpage_dag' AIRFLOW_CTX_TASK_ID='submit_spark_word_count_job' AIRFLOW_CTX_EXECUTION_DATE='2023-10-23T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-10-23T00:00:00+00:00'
[2023-10-25T18:23:39.058+0530] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-10-25T18:23:39.059+0530] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master yarn:587 --name PythonWordCount --queue root.default /home/harika/sparkjobs/word_count_extended.py /home/harika/webdata/web_page_data/content.txt
[2023-10-25T18:23:40.050+0530] {spark_submit.py:491} INFO - 23/10/25 18:23:40 WARN Utils: Your hostname, harika-Latitude-5511 resolves to a loopback address: 127.0.1.1; using 192.168.29.76 instead (on interface wlo1)
[2023-10-25T18:23:40.052+0530] {spark_submit.py:491} INFO - 23/10/25 18:23:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn:587' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1103)
[2023-10-25T18:23:40.107+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1103)
[2023-10-25T18:23:40.108+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
[2023-10-25T18:23:40.108+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
[2023-10-25T18:23:40.108+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
[2023-10-25T18:23:40.108+0530] {spark_submit.py:491} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-10-25T18:23:40.149+0530] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/harika/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook.submit(self._application)
  File "/home/harika/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn:587 --name PythonWordCount --queue root.default /home/harika/sparkjobs/word_count_extended.py /home/harika/webdata/web_page_data/content.txt. Error code is: 1.
[2023-10-25T18:23:40.155+0530] {taskinstance.py:1398} INFO - Marking task as UP_FOR_RETRY. dag_id=webpage_dag, task_id=submit_spark_word_count_job, execution_date=20231023T000000, start_date=20231025T125338, end_date=20231025T125340
[2023-10-25T18:23:40.173+0530] {standard_task_runner.py:104} ERROR - Failed to execute job 1595 for task submit_spark_word_count_job (Cannot execute: spark-submit --master yarn:587 --name PythonWordCount --queue root.default /home/harika/sparkjobs/word_count_extended.py /home/harika/webdata/web_page_data/content.txt. Error code is: 1.; 94077)
[2023-10-25T18:23:40.196+0530] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-10-25T18:23:40.213+0530] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check

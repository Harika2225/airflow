[2023-10-30T10:13:50.534+0530] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-29T00:00:00+00:00 [queued]>
[2023-10-30T10:13:50.541+0530] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-29T00:00:00+00:00 [queued]>
[2023-10-30T10:13:50.541+0530] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2023-10-30T10:13:50.551+0530] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): submit_spark_word_count_job> on 2023-10-29 00:00:00+00:00
[2023-10-30T10:13:50.556+0530] {standard_task_runner.py:57} INFO - Started process 14645 to run task
[2023-10-30T10:13:50.558+0530] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'webpage_dag', 'submit_spark_word_count_job', 'scheduled__2023-10-29T00:00:00+00:00', '--job-id', '1863', '--raw', '--subdir', 'DAGS_FOLDER/word_count_url.py', '--cfg-path', '/tmp/tmph4gyjavd']
[2023-10-30T10:13:50.559+0530] {standard_task_runner.py:85} INFO - Job 1863: Subtask submit_spark_word_count_job
[2023-10-30T10:13:50.586+0530] {task_command.py:415} INFO - Running <TaskInstance: webpage_dag.submit_spark_word_count_job scheduled__2023-10-29T00:00:00+00:00 [running]> on host harika-Latitude-5511
[2023-10-30T10:13:50.646+0530] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='harikasree2225@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='webpage_dag' AIRFLOW_CTX_TASK_ID='submit_spark_word_count_job' AIRFLOW_CTX_EXECUTION_DATE='2023-10-29T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-10-29T00:00:00+00:00'
[2023-10-30T10:13:50.651+0530] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-10-30T10:13:50.652+0530] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master local --name PythonWordCount --queue root.default /home/harika/sparkjobs/word_count_extended.py /home/harika/webdata/web_page_data/content.txt
[2023-10-30T10:13:51.878+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:51 WARN Utils: Your hostname, harika-Latitude-5511 resolves to a loopback address: 127.0.1.1; using 192.168.29.76 instead (on interface wlo1)
[2023-10-30T10:13:51.880+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-10-30T10:13:53.543+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SparkContext: Running Spark version 3.5.0
[2023-10-30T10:13:53.543+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SparkContext: OS info Linux, 6.2.0-35-generic, amd64
[2023-10-30T10:13:53.543+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SparkContext: Java version 17.0.1
[2023-10-30T10:13:53.603+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-10-30T10:13:53.710+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO ResourceUtils: ==============================================================
[2023-10-30T10:13:53.711+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-10-30T10:13:53.711+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO ResourceUtils: ==============================================================
[2023-10-30T10:13:53.712+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SparkContext: Submitted application: PythonWordCount
[2023-10-30T10:13:53.735+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-10-30T10:13:53.745+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO ResourceProfile: Limiting resource is cpu
[2023-10-30T10:13:53.746+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-10-30T10:13:53.818+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SecurityManager: Changing view acls to: harika
[2023-10-30T10:13:53.818+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SecurityManager: Changing modify acls to: harika
[2023-10-30T10:13:53.819+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SecurityManager: Changing view acls groups to:
[2023-10-30T10:13:53.819+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SecurityManager: Changing modify acls groups to:
[2023-10-30T10:13:53.820+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: harika; groups with view permissions: EMPTY; users with modify permissions: harika; groups with modify permissions: EMPTY
[2023-10-30T10:13:54.085+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Utils: Successfully started service 'sparkDriver' on port 34173.
[2023-10-30T10:13:54.116+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO SparkEnv: Registering MapOutputTracker
[2023-10-30T10:13:54.153+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO SparkEnv: Registering BlockManagerMaster
[2023-10-30T10:13:54.175+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-10-30T10:13:54.176+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-10-30T10:13:54.179+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-10-30T10:13:54.203+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-330b0629-005f-4f41-b705-808fe8f015f2
[2023-10-30T10:13:54.216+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-10-30T10:13:54.234+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-10-30T10:13:54.378+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-10-30T10:13:54.451+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-10-30T10:13:54.574+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Executor: Starting executor ID driver on host 192.168.29.76
[2023-10-30T10:13:54.574+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Executor: OS info Linux, 6.2.0-35-generic, amd64
[2023-10-30T10:13:54.575+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Executor: Java version 17.0.1
[2023-10-30T10:13:54.581+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-10-30T10:13:54.581+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@21ff27c1 for default.
[2023-10-30T10:13:54.607+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42745.
[2023-10-30T10:13:54.607+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO NettyBlockTransferService: Server created on 192.168.29.76:42745
[2023-10-30T10:13:54.609+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-10-30T10:13:54.615+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.29.76, 42745, None)
[2023-10-30T10:13:54.618+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.29.76:42745 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.29.76, 42745, None)
[2023-10-30T10:13:54.621+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.29.76, 42745, None)
[2023-10-30T10:13:54.623+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.29.76, 42745, None)
[2023-10-30T10:13:54.947+0530] {spark_submit.py:491} INFO - Input file location: /home/harika/webdata/web_page_data/content.txt
[2023-10-30T10:13:55.000+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-10-30T10:13:55.006+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:55 INFO SharedState: Warehouse path is 'file:/home/harika/spark-warehouse'.
[2023-10-30T10:13:55.760+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:55 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2023-10-30T10:13:56.383+0530] {spark_submit.py:491} INFO - Showing records of dataframe read from file
[2023-10-30T10:13:56.403+0530] {spark_submit.py:491} INFO - root
[2023-10-30T10:13:56.404+0530] {spark_submit.py:491} INFO - |-- value: string (nullable = true)
[2023-10-30T10:13:56.404+0530] {spark_submit.py:491} INFO - 
[2023-10-30T10:13:56.404+0530] {spark_submit.py:491} INFO - None
[2023-10-30T10:13:56.674+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:56 INFO FileSourceStrategy: Pushed Filters:
[2023-10-30T10:13:56.682+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:56 INFO FileSourceStrategy: Post-Scan Filters:
[2023-10-30T10:13:57.083+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO CodeGenerator: Code generated in 131.007663 ms
[2023-10-30T10:13:57.122+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.2 KiB, free 434.2 MiB)
[2023-10-30T10:13:57.171+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 434.2 MiB)
[2023-10-30T10:13:57.173+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.29.76:42745 (size: 34.2 KiB, free: 434.4 MiB)
[2023-10-30T10:13:57.176+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0
[2023-10-30T10:13:57.184+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212094 bytes, open cost is considered as scanning 4194304 bytes.
[2023-10-30T10:13:57.269+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2023-10-30T10:13:57.281+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-30T10:13:57.281+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2023-10-30T10:13:57.281+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Parents of final stage: List()
[2023-10-30T10:13:57.282+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Missing parents: List()
[2023-10-30T10:13:57.285+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-30T10:13:57.345+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.8 KiB, free 434.2 MiB)
[2023-10-30T10:13:57.351+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)
[2023-10-30T10:13:57.351+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.29.76:42745 (size: 6.1 KiB, free: 434.4 MiB)
[2023-10-30T10:13:57.352+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2023-10-30T10:13:57.365+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-30T10:13:57.366+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-10-30T10:13:57.397+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.29.76, executor driver, partition 0, PROCESS_LOCAL, 8231 bytes)
[2023-10-30T10:13:57.406+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-10-30T10:13:57.498+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO CodeGenerator: Code generated in 13.658144 ms
[2023-10-30T10:13:57.502+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO FileScanRDD: Reading File path: file:///home/harika/webdata/web_page_data/content.txt, range: 0-17790, partition values: [empty row]
[2023-10-30T10:13:57.521+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO CodeGenerator: Code generated in 12.868366 ms
[2023-10-30T10:13:57.566+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3484 bytes result sent to driver
[2023-10-30T10:13:57.577+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 187 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-30T10:13:57.578+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-10-30T10:13:57.582+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.288 s
[2023-10-30T10:13:57.584+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-10-30T10:13:57.584+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-10-30T10:13:57.586+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:57 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.316388 s
[2023-10-30T10:13:58.232+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO CodeGenerator: Code generated in 11.194701 ms
[2023-10-30T10:13:58.299+0530] {spark_submit.py:491} INFO - +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-30T10:13:58.299+0530] {spark_submit.py:491} INFO - |value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
[2023-10-30T10:13:58.300+0530] {spark_submit.py:491} INFO - +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-30T10:13:58.301+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.301+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.302+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.302+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.303+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.303+0530] {spark_submit.py:491} INFO - |Apache Spark: Introduction, Examples and Use Cases | Toptal®                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
[2023-10-30T10:13:58.304+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.304+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.305+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.305+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.306+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.306+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.307+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.307+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.308+0530] {spark_submit.py:491} INFO - |®Top 3%WhyClientsEnterpriseCommunityBlogAbout UsApply as a DeveloperHire a DeveloperLog In®DevelopersTop 3%WhyClientsEnterpriseCommunityBlogAbout UsApply as a DeveloperHire a DeveloperLog InEngineeringEngineeringDesignFinanceProjectsProductToptal InsightsSearchData Science and Databases8 minute readIntroduction to Apache Spark with Examples and Use CasesIn this post, Toptal engineer Radek Ostrowski introduces Apache Spark – fast, easy-to-use, and flexible big data processing.  Billed as offering “lightning fast cluster computing”, the Spark technology stack incorporates a comprehensive set of capabilities, including SparkSQL, Spark Streaming, MLlib (for machine learning), and GraphX. Spark may very well be the “child prodigy of big data”, rapidly gaining a dominant position in the complex world of big data processing.|
[2023-10-30T10:13:58.308+0530] {spark_submit.py:491} INFO - |Toptalauthors are vetted experts in their fields and write on topics in which they have demonstrated experience. All of our content is peer reviewed and validated by Toptal experts in the same field.In this post, Toptal engineer Radek Ostrowski introduces Apache Spark – fast, easy-to-use, and flexible big data processing.  Billed as offering “lightning fast cluster computing”, the Spark technology stack incorporates a comprehensive set of capabilities, including SparkSQL, Spark Streaming, MLlib (for machine learning), and GraphX. Spark may very well be the “child prodigy of big data”, rapidly gaining a dominant position in the complex world of big data processing.                                                                                                                                                             |
[2023-10-30T10:13:58.309+0530] {spark_submit.py:491} INFO - |Toptalauthors are vetted experts in their fields and write on topics in which they have demonstrated experience. All of our content is peer reviewed and validated by Toptal experts in the same field.By Radek OstrowskiVerified Expert in EngineeringRadek is a blockchain engineer with an interest in Ethereum smart contracts. He also has extensive experience in machine learning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
[2023-10-30T10:13:58.309+0530] {spark_submit.py:491} INFO - |ExpertiseSparkApacheBig DataPreviously AtShareShareI first heard of Spark in late 2013 when I became interested in Scala, the language in which Spark is written. Some time later, I did a fun data science project trying to predict survival on the Titanic. This turned out to be a great way to get further introduced to Spark concepts and programming. I highly recommend it for any aspiring Spark developers looking for a place to get started.                                                                                                                                                                                                                                                                                                                                                                                                    |
[2023-10-30T10:13:58.310+0530] {spark_submit.py:491} INFO - |Today, Spark is being adopted by major players like Amazon, eBay, and Yahoo! Many organizations run Spark on clusters with thousands of nodes. According to the Spark FAQ, the largest known cluster has over 8000 nodes. Indeed, Spark is a technology well worth taking note of and learning about.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
[2023-10-30T10:13:58.310+0530] {spark_submit.py:491} INFO - |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
[2023-10-30T10:13:58.311+0530] {spark_submit.py:491} INFO - +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-30T10:13:58.311+0530] {spark_submit.py:491} INFO - only showing top 20 rows
[2023-10-30T10:13:58.312+0530] {spark_submit.py:491} INFO - 
[2023-10-30T10:13:58.312+0530] {spark_submit.py:491} INFO - None
[2023-10-30T10:13:58.656+0530] {spark_submit.py:491} INFO - Records of dataframe after processing text df
[2023-10-30T10:13:58.656+0530] {spark_submit.py:491} INFO - root
[2023-10-30T10:13:58.657+0530] {spark_submit.py:491} INFO - |-- words_clean: array (nullable = true)
[2023-10-30T10:13:58.657+0530] {spark_submit.py:491} INFO - |    |-- element: string (containsNull = true)
[2023-10-30T10:13:58.657+0530] {spark_submit.py:491} INFO - 
[2023-10-30T10:13:58.657+0530] {spark_submit.py:491} INFO - None
[2023-10-30T10:13:58.708+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO FileSourceStrategy: Pushed Filters:
[2023-10-30T10:13:58.708+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO FileSourceStrategy: Post-Scan Filters:
[2023-10-30T10:13:58.772+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.2 KiB, free 434.0 MiB)
[2023-10-30T10:13:58.785+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.9 MiB)
[2023-10-30T10:13:58.786+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.29.76:42745 (size: 34.2 KiB, free: 434.3 MiB)
[2023-10-30T10:13:58.787+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[2023-10-30T10:13:58.788+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212094 bytes, open cost is considered as scanning 4194304 bytes.
[2023-10-30T10:13:58.809+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Registering RDD 7 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2023-10-30T10:13:58.813+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-30T10:13:58.813+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2023-10-30T10:13:58.814+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Parents of final stage: List()
[2023-10-30T10:13:58.814+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Missing parents: List()
[2023-10-30T10:13:58.816+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-30T10:13:58.822+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.29.76:42745 in memory (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-30T10:13:58.826+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.29.76:42745 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2023-10-30T10:13:58.840+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2023-10-30T10:13:58.844+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)
[2023-10-30T10:13:58.844+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.29.76:42745 (size: 6.1 KiB, free: 434.4 MiB)
[2023-10-30T10:13:58.845+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[2023-10-30T10:13:58.848+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-30T10:13:58.848+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-10-30T10:13:58.850+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.29.76, executor driver, partition 0, PROCESS_LOCAL, 8220 bytes)
[2023-10-30T10:13:58.851+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-10-30T10:13:58.886+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO FileScanRDD: Reading File path: file:///home/harika/webdata/web_page_data/content.txt, range: 0-17790, partition values: [empty row]
[2023-10-30T10:13:58.946+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1805 bytes result sent to driver
[2023-10-30T10:13:58.948+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 99 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-30T10:13:58.948+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-10-30T10:13:58.951+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.131 s
[2023-10-30T10:13:58.951+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: looking for newly runnable stages
[2023-10-30T10:13:58.952+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: running: Set()
[2023-10-30T10:13:58.952+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: waiting: Set()
[2023-10-30T10:13:58.952+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:58 INFO DAGScheduler: failed: Set()
[2023-10-30T10:13:59.023+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 30.934713 ms
[2023-10-30T10:13:59.087+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2023-10-30T10:13:59.089+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-30T10:13:59.089+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2023-10-30T10:13:59.089+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2023-10-30T10:13:59.089+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Missing parents: List()
[2023-10-30T10:13:59.090+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-30T10:13:59.104+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 39.2 KiB, free 434.1 MiB)
[2023-10-30T10:13:59.110+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 434.1 MiB)
[2023-10-30T10:13:59.110+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.29.76:42745 (size: 17.6 KiB, free: 434.3 MiB)
[2023-10-30T10:13:59.111+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.29.76:42745 in memory (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-30T10:13:59.111+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
[2023-10-30T10:13:59.112+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-30T10:13:59.112+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-10-30T10:13:59.118+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (192.168.29.76, executor driver, partition 0, NODE_LOCAL, 7615 bytes)
[2023-10-30T10:13:59.119+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
[2023-10-30T10:13:59.173+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO ShuffleBlockFetcherIterator: Getting 1 (3.5 KiB) non-empty blocks including 1 (3.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-30T10:13:59.174+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2023-10-30T10:13:59.200+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 22.355533 ms
[2023-10-30T10:13:59.217+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 7.020874 ms
[2023-10-30T10:13:59.246+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 21.459654 ms
[2023-10-30T10:13:59.261+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 10.814508 ms
[2023-10-30T10:13:59.277+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4211 bytes result sent to driver
[2023-10-30T10:13:59.279+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 162 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-30T10:13:59.279+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-10-30T10:13:59.280+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.183 s
[2023-10-30T10:13:59.280+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-10-30T10:13:59.280+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-10-30T10:13:59.281+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.193629 s
[2023-10-30T10:13:59.330+0530] {spark_submit.py:491} INFO - +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-30T10:13:59.330+0530] {spark_submit.py:491} INFO - |words_clean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
[2023-10-30T10:13:59.331+0530] {spark_submit.py:491} INFO - +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-30T10:13:59.331+0530] {spark_submit.py:491} INFO - |[spark, core, base, engine, largescale, parallel, distributed, data, processing, responsible]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
[2023-10-30T10:13:59.331+0530] {spark_submit.py:491} INFO - |[toptalauthors, vetted, experts, fields, write, topics, demonstrated, experience, content, peer, reviewed, validated, toptal, experts, fieldin, post, toptal, engineer, radek, ostrowski, introduces, apache, spark, , fast, easytouse, flexible, big, data, processing, , billed, offering, lightning, fast, cluster, computing, spark, technology, stack, incorporates, comprehensive, set, capabilities, including, sparksql, spark, streaming, mllib, machine, learning, graphx, spark, may, well, child, prodigy, big, data, rapidly, gaining, dominant, position, complex, world, big, dataprocessing]|
[2023-10-30T10:13:59.331+0530] {spark_submit.py:491} INFO - |[spark, introduces, concept, rdd, resilient, distributed, dataset, immutable, faulttolerant, distributed, collection, objects, operated, parallel, rdd, contain, type, object, created, loading, external, dataset, distributing, collection, driver, program]                                                                                                                                                                                                                                                                                                                                              |
[2023-10-30T10:13:59.331+0530] {spark_submit.py:491} INFO - |[, get, evaluation, metrics]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
[2023-10-30T10:13:59.331+0530] {spark_submit.py:491} INFO - |[sqlcontextsqlload, data, local, inpath, examplessrcmainresourceskvtxt, table, src]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
[2023-10-30T10:13:59.332+0530] {spark_submit.py:491} INFO - |[val, model, , svmwithsgdtraintraining, numiterations]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
[2023-10-30T10:13:59.332+0530] {spark_submit.py:491} INFO - |[apache, spark, introduction, examples, use, cases, , toptal]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
[2023-10-30T10:13:59.332+0530] {spark_submit.py:491} INFO - |[, , , , , , , , , , collectforeachsendemail]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
[2023-10-30T10:13:59.332+0530] {spark_submit.py:491} INFO - |[val, test, , splits]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
[2023-10-30T10:13:59.332+0530] {spark_submit.py:491} INFO - |[sparksql]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
[2023-10-30T10:13:59.332+0530] {spark_submit.py:491} INFO - |[val, sqlcontext, , new, orgapachesparksqlhivehivecontextsc]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.333+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.334+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.334+0530] {spark_submit.py:491} INFO - |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2023-10-30T10:13:59.334+0530] {spark_submit.py:491} INFO - +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2023-10-30T10:13:59.334+0530] {spark_submit.py:491} INFO - only showing top 20 rows
[2023-10-30T10:13:59.334+0530] {spark_submit.py:491} INFO - 
[2023-10-30T10:13:59.334+0530] {spark_submit.py:491} INFO - None
[2023-10-30T10:13:59.347+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO FileSourceStrategy: Pushed Filters:
[2023-10-30T10:13:59.348+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO FileSourceStrategy: Post-Scan Filters:
[2023-10-30T10:13:59.358+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.2 KiB, free 433.9 MiB)
[2023-10-30T10:13:59.368+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.29.76:42745 in memory (size: 17.6 KiB, free: 434.4 MiB)
[2023-10-30T10:13:59.376+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.9 MiB)
[2023-10-30T10:13:59.376+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.29.76:42745 (size: 34.2 KiB, free: 434.3 MiB)
[2023-10-30T10:13:59.377+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0
[2023-10-30T10:13:59.378+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212094 bytes, open cost is considered as scanning 4194304 bytes.
[2023-10-30T10:13:59.383+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Registering RDD 14 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2023-10-30T10:13:59.383+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Got map stage job 3 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-10-30T10:13:59.383+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0)
[2023-10-30T10:13:59.383+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Parents of final stage: List()
[2023-10-30T10:13:59.383+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Missing parents: List()
[2023-10-30T10:13:59.384+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-10-30T10:13:59.386+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2023-10-30T10:13:59.390+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 433.9 MiB)
[2023-10-30T10:13:59.391+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.29.76:42745 (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-30T10:13:59.391+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[2023-10-30T10:13:59.392+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-10-30T10:13:59.392+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-10-30T10:13:59.393+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (192.168.29.76, executor driver, partition 0, PROCESS_LOCAL, 8220 bytes)
[2023-10-30T10:13:59.394+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[2023-10-30T10:13:59.399+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO FileScanRDD: Reading File path: file:///home/harika/webdata/web_page_data/content.txt, range: 0-17790, partition values: [empty row]
[2023-10-30T10:13:59.412+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1805 bytes result sent to driver
[2023-10-30T10:13:59.414+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 21 ms on 192.168.29.76 (executor driver) (1/1)
[2023-10-30T10:13:59.414+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-10-30T10:13:59.415+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.030 s
[2023-10-30T10:13:59.415+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: looking for newly runnable stages
[2023-10-30T10:13:59.415+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: running: Set()
[2023-10-30T10:13:59.415+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: waiting: Set()
[2023-10-30T10:13:59.416+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: failed: Set()
[2023-10-30T10:13:59.444+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 20.646618 ms
[2023-10-30T10:13:59.528+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO SparkContext: Starting job: countByValue at /home/harika/sparkjobs/word_count_extended.py:53
[2023-10-30T10:13:59.530+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Got job 4 (countByValue at /home/harika/sparkjobs/word_count_extended.py:53) with 4 output partitions
[2023-10-30T10:13:59.530+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Final stage: ResultStage 6 (countByValue at /home/harika/sparkjobs/word_count_extended.py:53)
[2023-10-30T10:13:59.530+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2023-10-30T10:13:59.530+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Missing parents: List()
[2023-10-30T10:13:59.531+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[20] at countByValue at /home/harika/sparkjobs/word_count_extended.py:53), which has no missing parents
[2023-10-30T10:13:59.535+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 47.4 KiB, free 433.9 MiB)
[2023-10-30T10:13:59.540+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 433.9 MiB)
[2023-10-30T10:13:59.541+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.29.76:42745 (size: 22.1 KiB, free: 434.3 MiB)
[2023-10-30T10:13:59.541+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
[2023-10-30T10:13:59.541+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.29.76:42745 in memory (size: 6.1 KiB, free: 434.3 MiB)
[2023-10-30T10:13:59.542+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 6 (PythonRDD[20] at countByValue at /home/harika/sparkjobs/word_count_extended.py:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2023-10-30T10:13:59.542+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSchedulerImpl: Adding task set 6.0 with 4 tasks resource profile 0
[2023-10-30T10:13:59.543+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (192.168.29.76, executor driver, partition 0, NODE_LOCAL, 7615 bytes)
[2023-10-30T10:13:59.544+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)
[2023-10-30T10:13:59.564+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO ShuffleBlockFetcherIterator: Getting 1 (3.5 KiB) non-empty blocks including 1 (3.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-30T10:13:59.565+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-10-30T10:13:59.581+0530] {spark_submit.py:491} INFO - 23/10/30 10:13:59 INFO CodeGenerator: Code generated in 16.219353 ms
[2023-10-30T10:14:00.244+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO PythonRunner: Times: total = 652, boot = 536, init = 115, finish = 1
[2023-10-30T10:14:00.250+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 7327 bytes result sent to driver
[2023-10-30T10:14:00.251+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 5) (192.168.29.76, executor driver, partition 1, NODE_LOCAL, 7615 bytes)
[2023-10-30T10:14:00.251+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 708 ms on 192.168.29.76 (executor driver) (1/4)
[2023-10-30T10:14:00.251+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Running task 1.0 in stage 6.0 (TID 5)
[2023-10-30T10:14:00.253+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37927
[2023-10-30T10:14:00.260+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO ShuffleBlockFetcherIterator: Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-30T10:14:00.260+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-10-30T10:14:00.344+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO PythonRunner: Times: total = 83, boot = -7, init = 89, finish = 1
[2023-10-30T10:14:00.351+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Finished task 1.0 in stage 6.0 (TID 5). 8806 bytes result sent to driver
[2023-10-30T10:14:00.351+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 6) (192.168.29.76, executor driver, partition 2, NODE_LOCAL, 7615 bytes)
[2023-10-30T10:14:00.352+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Running task 2.0 in stage 6.0 (TID 6)
[2023-10-30T10:14:00.352+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 5) in 102 ms on 192.168.29.76 (executor driver) (2/4)
[2023-10-30T10:14:00.363+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-30T10:14:00.363+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-10-30T10:14:00.448+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO PythonRunner: Times: total = 84, boot = -5, init = 89, finish = 0
[2023-10-30T10:14:00.456+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Finished task 2.0 in stage 6.0 (TID 6). 6881 bytes result sent to driver
[2023-10-30T10:14:00.457+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 7) (192.168.29.76, executor driver, partition 3, NODE_LOCAL, 7615 bytes)
[2023-10-30T10:14:00.458+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 6) in 106 ms on 192.168.29.76 (executor driver) (3/4)
[2023-10-30T10:14:00.458+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Running task 3.0 in stage 6.0 (TID 7)
[2023-10-30T10:14:00.468+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO ShuffleBlockFetcherIterator: Getting 1 (4.3 KiB) non-empty blocks including 1 (4.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-10-30T10:14:00.468+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-10-30T10:14:00.590+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO PythonRunner: Times: total = 120, boot = -8, init = 127, finish = 1
[2023-10-30T10:14:00.592+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO Executor: Finished task 3.0 in stage 6.0 (TID 7). 8737 bytes result sent to driver
[2023-10-30T10:14:00.593+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 7) in 137 ms on 192.168.29.76 (executor driver) (4/4)
[2023-10-30T10:14:00.594+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-10-30T10:14:00.595+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO DAGScheduler: ResultStage 6 (countByValue at /home/harika/sparkjobs/word_count_extended.py:53) finished in 1.063 s
[2023-10-30T10:14:00.596+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-10-30T10:14:00.596+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-10-30T10:14:00.596+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO DAGScheduler: Job 4 finished: countByValue at /home/harika/sparkjobs/word_count_extended.py:53, took 1.068177 s
[2023-10-30T10:14:00.607+0530] {spark_submit.py:491} INFO - defaultdict(<class 'int'>, {'spark': 65, 'core': 4, 'base': 1, 'engine': 4, 'largescale': 1, 'parallel': 2, 'distributed': 3, 'data': 30, 'processing': 9, 'responsible': 2, 'toptalauthors': 2, 'vetted': 3, 'experts': 6, 'fields': 3, 'write': 4, 'topics': 3, 'demonstrated': 3, 'experience': 6, 'content': 3, 'peer': 3, 'reviewed': 3, 'validated': 3, 'toptal': 10, 'fieldin': 1, 'post': 2, 'engineer': 5, 'radek': 3, 'ostrowski': 2, 'introduces': 3, 'apache': 21, '': 190, 'fast': 6, 'easytouse': 2, 'flexible': 2, 'big': 10, 'billed': 2, 'offering': 2, 'lightning': 4, 'cluster': 8, 'computing': 4, 'technology': 4, 'stack': 4, 'incorporates': 2, 'comprehensive': 2, 'set': 4, 'capabilities': 3, 'including': 3, 'sparksql': 6, 'streaming': 14, 'mllib': 7, 'machine': 10, 'learning': 13, 'graphx': 5, 'may': 4, 'well': 8, 'child': 2, 'prodigy': 2, 'rapidly': 2, 'gaining': 2, 'dominant': 2, 'position': 2, 'complex': 3, 'world': 3, 'dataprocessing': 2, 'concept': 1, 'rdd': 7, 'resilient': 1, 'dataset': 3, 'immutable': 1, 'faulttolerant': 1, 'collection': 2, 'objects': 1, 'operated': 1, 'contain': 1, 'type': 1, 'object': 1, 'created': 1, 'loading': 1, 'external': 2, 'distributing': 2, 'driver': 2, 'program': 2, 'get': 4, 'evaluation': 1, 'metrics': 2, 'sqlcontextsqlload': 1, 'local': 1, 'inpath': 1, 'examplessrcmainresourceskvtxt': 1, 'table': 3, 'src': 3, 'val': 12, 'model': 3, 'svmwithsgdtraintraining': 1, 'numiterations': 2, 'introduction': 3, 'examples': 3, 'use': 13, 'cases': 5, 'collectforeachsendemail': 1, 'test': 4, 'splits': 2, 'sqlcontext': 2, 'new': 5, 'orgapachesparksqlhivehivecontextsc': 2, 'library': 4, 'provides': 7, 'various': 4, 'algorithms': 4, 'designed': 1, 'scale': 1, 'classification': 1, 'regression': 2, 'clustering': 3, 'collaborative': 2, 'filtering': 2, 'check': 2, 'toptals': 1, 'article': 6, 'information': 5, 'topic': 1, 'also': 9, 'work': 3, 'linear': 1, 'using': 4, 'ordinary': 1, 'least': 1, 'squares': 1, 'kmeans': 2, 'way': 3, 'mahout': 1, 'hadoop': 5, 'already': 1, 'turned': 2, 'away': 2, 'mapreduce': 3, 'joined': 1, 'forces': 1, 'project': 8, 'advertised': 2, 'thriving': 2, 'opensource': 2, 'community': 2, 'active': 2, 'moment': 2, 'integrates': 1, 'ecosystem': 1, 'sources': 4, 'hdfs': 1, 'amazon': 2, 'hive': 5, 'hbase': 1, 'cassandra': 1, 'etc': 1, 'sendemail': 1, 'custom': 1, 'function': 1, 'interacting': 1, 'storage': 1, 'systems': 1, 'auroc': 2, 'metricsareaunderroc': 1, 'youspark': 1, 'helps': 2, 'simplify': 2, 'challenging': 2, 'computationally': 2, 'intensive': 2, 'task': 2, 'high': 2, 'volumes': 2, 'realtime': 4, 'archived': 3, 'reading': 1, 'bloghow': 1, 'used': 6, 'docker': 1, 'hackathon': 1, 'build': 2, 'weather': 1, 'appapache': 1, 'tutorial': 1, 'identifying': 1, 'trending': 2, 'twitter': 4, 'hashtagsai': 1, 'development': 2, 'managers': 1, 'preparestreamline': 1, 'software': 1, 'integration': 1, 'camel': 1, 'tutorialthe': 1, 'ultimate': 1, 'ens': 1, 'app': 2, 'tutorialunderstanding': 1, 'basicswhat': 1, 'sparkspark': 1, 'actions': 1, 'operations': 5, 'reduce': 1, 'count': 2, 'first': 6, 'return': 2, 'value': 2, 'running': 1, 'computation': 1, 'mapword': 1, 'word': 2, 'reducebykey': 1, 'transformations': 4, 'lazy': 1, 'meaning': 1, 'compute': 2, 'results': 5, 'right': 1, 'instead': 1, 'remember': 1, 'operation': 2, 'performed': 3, 'eg': 2, 'file': 3, 'actually': 1, 'computed': 1, 'action': 3, 'called': 1, 'result': 3, 'returned': 1, 'design': 1, 'enables': 1, 'run': 9, 'efficiently': 1, 'example': 7, 'transformed': 2, 'ways': 1, 'passed': 2, 'process': 1, 'line': 2, 'rather': 1, 'entire': 2, 'answered': 1, 'question': 1, 'lets': 3, 'think': 2, 'kind': 1, 'problems': 1, 'challenges': 1, 'effectively': 1, 'tagsbigdatasparkapacheconsult': 1, 'author': 2, 'expert': 2, 'thistopicschedule': 2, 'callradek': 1, 'ostrowskiverified': 3, 'expertin': 3, 'engineeringlocated': 1, 'phuket': 1, 'thailandmember': 1, 'since': 1, 'september': 1, 'authorradek': 1, 'blockchain': 2, 'interest': 2, 'ethereum': 2, 'smart': 2, 'contracts': 2, 'extensive': 2, 'faster': 5, 'general': 1, 'platform': 1, 'programs': 1, 'x': 3, 'memory': 3, 'disk': 1, 'last': 1, 'year': 1, 'took': 1, 'completing': 1, 'tb': 1, 'daytona': 1, 'graysort': 1, 'contest': 1, 'one': 3, 'tenth': 1, 'number': 2, 'machines': 1, 'became': 2, 'fastest': 1, 'open': 1, 'source': 1, 'sorting': 1, 'petabyte': 1, 'game': 1, 'industry': 3, 'discovering': 1, 'patterns': 1, 'potential': 2, 'firehose': 1, 'ingame': 1, 'events': 1, 'able': 1, 'respond': 1, 'immediately': 1, 'capability': 1, 'yield': 2, 'lucrative': 1, 'business': 1, 'purposes': 1, 'player': 1, 'retention': 1, 'targeted': 1, 'advertising': 1, 'autoadjustment': 1, 'complexity': 1, 'level': 1, 'finance': 2, 'security': 2, 'applied': 1, 'fraud': 1, 'intrusion': 1, 'detection': 3, 'system': 1, 'riskbased': 1, 'authentication': 1, 'achieve': 1, 'topnotch': 1, 'harvesting': 1, 'huge': 1, 'amounts': 1, 'logs': 1, 'combining': 1, 'like': 10, 'breaches': 1, 'compromised': 1, 'accounts': 1, 'see': 2, 'httpshaveibeenpwnedcom': 1, 'connectionrequest': 1, 'ip': 1, 'geolocation': 1, 'time': 7, 'supports': 2, 'real': 1, 'production': 1, 'web': 1, 'server': 1, 'log': 1, 'files': 1, 'flume': 1, 'hdfss': 1, 'social': 1, 'media': 1, 'messaging': 1, 'queues': 1, 'kafka': 1, 'hood': 1, 'receives': 1, 'input': 1, 'streams': 1, 'divides': 1, 'batches': 2, 'next': 3, 'processed': 1, 'generate': 1, 'final': 1, 'stream': 2, 'depicted': 1, 'scheduling': 1, 'monitoring': 1, 'jobs': 1, 'clear': 1, 'default': 2, 'threshold': 1, 'score': 2, 'pointlabel': 1, 'ecommerce': 1, 'transaction': 1, 'algorithm': 2, 'als': 1, 'even': 2, 'combined': 1, 'unstructured': 2, 'customer': 1, 'comments': 1, 'product': 2, 'reviews': 1, 'constantly': 1, 'improve': 1, 'adapt': 1, 'recommendations': 1, 'trends': 1, 'saveastextfilehdfs': 1, 'heres': 1, 'quick': 1, 'certainly': 1, 'nowhere': 1, 'near': 1, 'exhaustive': 1, 'sampling': 1, 'require': 1, 'dealing': 1, 'velocity': 1, 'variety': 1, 'volume': 1, 'suited': 1, 'authors': 2, 'fieldexpertisesparkapachebig': 1, 'datapreviously': 2, 'athire': 1, 'radekworldclass': 1, 'articles': 3, 'delivered': 3, 'weeklyget': 1, 'great': 3, 'contentsubscription': 1, 'implies': 3, 'consent': 3, 'privacy': 3, 'policy': 3, 'twitterutilscreatestream': 1, 'another': 1, 'important': 1, 'aspect': 1, 'interactive': 1, 'shell': 1, 'repl': 2, 'outofthe': 1, 'box': 1, 'outcome': 1, 'code': 10, 'without': 2, 'needing': 1, 'execute': 1, 'job': 1, 'path': 1, 'working': 1, 'thus': 1, 'much': 2, 'shorter': 1, 'adhoc': 1, 'analysis': 5, 'made': 1, 'possible': 3, 'additional': 2, 'key': 3, 'features': 1, 'include': 2, 'semantic': 1, 'tweets': 6, 'determine': 1, 'appear': 1, 'referencing': 1, 'current': 1, 'earthquake': 11, 'occurrence': 1, 'shaking': 2, 'consider': 1, 'positive': 2, 'matches': 2, 'whereas': 2, 'attending': 1, 'conference': 1, 'yesterday': 1, 'scary': 1, 'paper': 1, 'support': 4, 'vector': 1, 'svm': 1, 'purpose': 2, 'try': 1, 'version': 1, 'resulting': 1, 'look': 2, 'following': 1, 'top': 3, 'whyclientsenterprisecommunityblogabout': 2, 'usapply': 2, 'developerhire': 2, 'developerlog': 2, 'indeveloperstop': 1, 'inengineeringengineeringdesignfinanceprojectsproducttoptal': 1, 'insightssearchdata': 1, 'science': 4, 'databases': 1, 'minute': 1, 'readintroduction': 1, 'casesin': 1, 'sqlcontextsqlcreate': 1, 'exists': 1, 'int': 1, 'string': 1, 'makes': 2, 'quickly': 1, 'highlevel': 1, 'operators': 1, 'disposal': 1, 'demonstrate': 1, 'hello': 1, 'bigdata': 1, 'written': 4, 'java': 2, 'around': 2, 'lines': 1, 'scala': 4, 'simply': 1, 'flatmapline': 1, 'linesplit': 1, 'split': 1, 'training': 3, 'filter': 2, 'seem': 1, 'relevant': 2, 'easily': 1, 'follows': 2, 'recomputed': 1, 'however': 1, 'persist': 2, 'cache': 1, 'method': 1, 'case': 2, 'keep': 1, 'elements': 1, 'access': 1, 'query': 4, 'manipulating': 1, 'graphs': 1, 'performing': 1, 'graphparallel': 1, 'uniform': 1, 'tool': 2, 'etl': 1, 'exploratory': 1, 'iterative': 1, 'graph': 4, 'computations': 1, 'apart': 1, 'builtin': 1, 'manipulation': 1, 'common': 1, 'pagerank': 1, 'sparkcontexttextfilehdfs': 1, 'expertisesparkapachebig': 1, 'atsharesharei': 1, 'heard': 1, 'late': 1, 'interested': 2, 'language': 3, 'later': 1, 'fun': 1, 'trying': 1, 'predict': 1, 'survival': 1, 'titanic': 1, 'introduced': 1, 'concepts': 1, 'programming': 1, 'highly': 1, 'recommend': 1, 'aspiring': 1, 'developers': 1, 'looking': 1, 'place': 2, 'started': 1, 'filtergettextcontainsearthquake': 1, 'gettextcontainsshaking': 1, 'splitscache': 1, 'came': 1, 'across': 1, 'recently': 1, 'experiment': 1, 'detect': 2, 'analyzing': 1, 'interestingly': 1, 'shown': 1, 'technique': 1, 'likely': 1, 'inform': 1, 'japan': 2, 'quicker': 1, 'meteorological': 1, 'agency': 1, 'though': 1, 'different': 1, 'put': 1, 'simplified': 1, 'snippets': 1, 'glue': 1, 'event': 1, 'mlutilsloadlibsvmfilesc': 1, 'sampleearthquatetweetstxt': 1, 'sqlcontextsqlfrom': 2, 'earthquakewarningusers': 1, 'select': 2, 'firstname': 1, 'lastname': 1, 'city': 1, 'email': 3, 'clusters': 2, 'managed': 1, 'yarn': 1, 'mesos': 1, 'standalone': 1, 'inapache': 1, 'datarandomsplitarray': 1, 'seed': 1, 'l': 1, 'sum': 1, 'structured': 1, 'seamlessly': 2, 'integrating': 1, 'brings': 1, 'masses': 1, 'raw': 1, 'scores': 1, 'modelclearthreshold': 1, 'extend': 1, 'far': 1, 'beyond': 1, 'earthquakes': 1, 'course': 1, 'today': 1, 'adopted': 1, 'major': 1, 'players': 1, 'ebay': 1, 'yahoo': 1, 'many': 1, 'organizations': 1, 'thousands': 1, 'nodes': 2, 'according': 1, 'faq': 1, 'largest': 1, 'known': 1, 'indeed': 1, 'worth': 1, 'taking': 1, 'note': 2, 'scoreandlabels': 1, 'testmap': 1, 'point': 1, 'sc': 2, 'existing': 3, 'sparkcontext': 2, 'queries': 2, 'expressed': 1, 'hiveql': 1, 'happy': 1, 'prediction': 1, 'rate': 1, 'move': 1, 'onto': 1, 'stage': 1, 'react': 2, 'whenever': 1, 'discover': 1, 'need': 1, 'certain': 1, 'ie': 1, 'density': 1, 'defined': 1, 'window': 1, 'described': 1, 'location': 2, 'services': 1, 'enabled': 1, 'extract': 1, 'armed': 1, 'knowledge': 1, 'storing': 1, 'users': 1, 'receiving': 1, 'notifications': 1, 'retrieve': 1, 'addresses': 1, 'send': 1, 'personalized': 1, 'warning': 1, 'worldclass': 1, 'weeklysign': 2, 'upsubscription': 2, 'conclusion': 1, 'developersalgorithm': 1, 'developersangular': 1, 'developersaws': 1, 'developersazure': 1, 'developersbig': 2, 'architectsblockchain': 1, 'developersbusiness': 1, 'intelligence': 1, 'developersc': 1, 'developerscomputer': 1, 'vision': 1, 'developersdjango': 1, 'developersdocker': 1, 'developerselixir': 1, 'developersgo': 1, 'engineersgraphql': 1, 'developersjenkins': 1, 'developerskotlin': 1, 'developerskubernetes': 1, 'expertsmachine': 1, 'engineersmagento': 1, 'developersnet': 1, 'developersr': 1, 'developersreact': 1, 'native': 1, 'developersruby': 1, 'rails': 1, 'developerssalesforce': 1, 'developerssql': 1, 'developerssys': 1, 'adminstableau': 1, 'developersunreal': 1, 'developersxamarin': 1, 'developersview': 1, 'freelancedevelopersjoin': 1, 'communityhire': 1, 'developer': 1, 'apply': 1, 'developerfooterondemand': 1, 'talenthire': 1, 'freelance': 5, 'developershire': 1, 'designershire': 1, 'expertshire': 1, 'managershire': 1, 'managersstart': 1, 'finish': 1, 'solutionsmanaged': 1, 'deliverymanagement': 1, 'consultingstrategy': 1, 'consultingpeople': 1, 'organization': 1, 'consultinginnovation': 1, 'consultingtechnology': 1, 'servicesapplication': 1, 'servicescloud': 1, 'servicesinformation': 1, 'servicesquality': 1, 'assurance': 1, 'servicesabouttop': 1, 'clientsfreelance': 1, 'jobsspecialized': 1, 'servicesutilities': 1, 'toolsresearch': 1, 'centerabout': 1, 'uscontactcontact': 1, 'uspress': 1, 'centercareersfaqthe': 1, 'worlds': 2, 'talent': 1, 'demandcopyright': 1, 'llcprivacy': 1, 'policywebsite': 1, 'termsaccessibility': 1, 'articlesengineeringdata': 1, 'databasesadvantages': 1, 'ai': 2, 'gpt': 2, 'diffusion': 1, 'models': 2, 'image': 1, 'generationengineeringdata': 1, 'databasesask': 1, 'nlp': 1, 'ethics': 2, 'aiengineeringtechnologyhow': 1, 'jwt': 1, 'nodejs': 1, 'better': 1, 'securityengineeringweb': 1, 'frontendnextjs': 1, 'vs': 1, 'comparative': 1, 'tutorialsee': 1, 'related': 1, 'talentspark': 1, 'developersapache': 1, 'architectsconsult': 1, 'callhire': 1, 'talentradek': 1, 'engineeringread': 1, 'nextengineeringtechnology': 1, 'pillars': 1, 'generative': 1, 'futureworldclass': 1, 'printlnarea': 1, 'roc': 1, 'binaryclassificationmetricsscoreandlabels': 1, 'map': 1, 'join': 1, 'union': 1, 'containing': 1, 'management': 1, 'fault': 1, 'recovery': 1, 'fieldby': 1, 'engineeringradek': 1, 'valuecollectforeachprintln': 1, 'component': 1, 'querying': 1, 'either': 1, 'via': 2, 'sql': 2, 'originated': 1, 'port': 1, 'integrated': 1, 'addition': 1, 'providing': 1, 'weave': 1, 'powerful': 2, 'compatible': 1, 'modelpredictpointfeatures': 1, 'api': 1, 'closely': 1, 'making': 1, 'easy': 1, 'programmers': 1, 'batch': 1, 'complemented': 1, 'higherlevel': 1, 'libraries': 3, 'application': 1, 'currently': 3, 'detailed': 1, 'extensions': 1, 'rdds': 1, 'two': 1, 'types': 1, 'prepare': 1, 'tweet': 1, 'load': 1, 'libsvm': 1, 'format': 1, 'contains': 1, 'website': 1, 'book': 1, 'lightningfast': 1, 'apis': 1, 'python': 1, 'languages': 1, 'r': 1})
[2023-10-30T10:14:00.607+0530] {spark_submit.py:491} INFO - Writing word counts to CSV file
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - word  count
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 0                190
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 1       spark     65
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 2        data     30
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 3      apache     21
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 4   streaming     14
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 5    learning     13
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 6         use     13
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 7         val     12
[2023-10-30T10:14:00.611+0530] {spark_submit.py:491} INFO - 8  earthquake     11
[2023-10-30T10:14:00.612+0530] {spark_submit.py:491} INFO - 9      toptal     10
[2023-10-30T10:14:00.612+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-10-30T10:14:00.621+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO SparkUI: Stopped Spark web UI at http://192.168.29.76:4040
[2023-10-30T10:14:00.631+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-10-30T10:14:00.640+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO MemoryStore: MemoryStore cleared
[2023-10-30T10:14:00.640+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO BlockManager: BlockManager stopped
[2023-10-30T10:14:00.643+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-10-30T10:14:00.645+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-10-30T10:14:00.649+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:00 INFO SparkContext: Successfully stopped SparkContext
[2023-10-30T10:14:01.692+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:01 INFO ShutdownHookManager: Shutdown hook called
[2023-10-30T10:14:01.692+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-c441f604-a738-46c5-861e-02ae111b391f
[2023-10-30T10:14:01.695+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-ecdc5983-29c8-4925-8018-8dd1bc347774
[2023-10-30T10:14:01.697+0530] {spark_submit.py:491} INFO - 23/10/30 10:14:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-ecdc5983-29c8-4925-8018-8dd1bc347774/pyspark-d1e0fb99-05c1-4538-99fb-ebee78581a7b
[2023-10-30T10:14:01.767+0530] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=webpage_dag, task_id=submit_spark_word_count_job, execution_date=20231029T000000, start_date=20231030T044350, end_date=20231030T044401
[2023-10-30T10:14:01.804+0530] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-10-30T10:14:01.820+0530] {taskinstance.py:2776} INFO - 1 downstream tasks scheduled from follow-on schedule check
